{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:  2.1.1+cpu\n",
      "Transformers version:  4.35.0\n"
     ]
    }
   ],
   "source": [
    "# you may need to `pip install upgrade transformers`\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will downlaod the model from huggingface.co and cache locally\n",
    "# run this block once to download the model\n",
    "\n",
    "# https://huggingface.co/AI-Sweden-Models/gpt-sw3-126m\n",
    "model_name = 'AI-Sweden-Models/gpt-sw3-126m-instruct'\n",
    "\n",
    "# download the model and tokenizer from huggingface.co and cache locally\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# save the model and tokenizer locally\n",
    "MODEL_PATH = \"gptsw3model126m\"\n",
    "model.save_pretrained(MODEL_PATH)\n",
    "tokenizer.save_pretrained(MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(64000, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=64000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model and tokenizer from local cache\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Som språkmodell är jag inte en AI, men jag kan ge dig en mängd information om livet och dess betydelse. För att få en bättre förståelse av livet, kan jag ge dig en kort beskrivning av livet och dess betydelse.\n",
      "\n",
      "Livet är ett komplext och mångfacetterat tillstånd som involverar många aspekter. Det är ett komplext och mångfacetterat tillstånd som involverar många aspekter, inklusive familj, utbildning, ekonomi, relationer, och så vidare. Om du vill förstå\n"
     ]
    }
   ],
   "source": [
    "\n",
    "promptInput = \"vad är meningen med livet?\"\n",
    "\n",
    "# this will be different for each model\n",
    "prompt = f\"\"\"\n",
    "<|endoftext|><s>\n",
    "User:\n",
    "{promptInput}\n",
    "\n",
    "<s>\n",
    "Bot:\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "generated_token_ids = model.generate(\n",
    "    inputs=input_ids,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=1,\n",
    ")[0]\n",
    "\n",
    "generated_text = tokenizer.decode(generated_token_ids)    \n",
    "\n",
    "\n",
    "\n",
    "# code to extract text after \"Bot:\"\n",
    "bot_text_start = generated_text.find(\"Bot:\")\n",
    "if bot_text_start != -1:\n",
    "    bot_text = generated_text[bot_text_start + len(\"Bot:\"):].strip()\n",
    "    # stop after <s>\n",
    "    bot_text = bot_text.split(\"<s>\")[0].strip()\n",
    "    print(bot_text)\n",
    "else:\n",
    "    print(\"Bot response not found in the generated text.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
